---
title: "Caret vs. tidymodels - comparing the old and new"
author: "Konrad Semsch"
date: "2019-08-06"
keywords: tech
slug: caret-vs-tidymodels-comparing-the-old-and-new
tags:
- caret
- tidymodels
- predictive modelling
categories: predictive modelling
---



<div id="summary" class="section level1">
<h1>Summary</h1>
<p>In this post I will make a short comparison between the most popular (by number of monthly downloads) ML framework available for R: <code>caret</code> and it’s successors being written by the same author (Max Kuhn) that are wrapped together in so called <code>tidymodels</code>. Tidymodels is a collections of different packages such as: <code>rsample</code>, <code>recipes</code>, <code>parsnip</code>, <code>dials</code> etc.</p>
<p>Many of them are still in a development phase which will still take a couple good months before they settle down so I’ll try to keep this post up-to-date. Nevertheless, I’ve wanted to take a closer look at what <code>tidymodels</code> have to offer for a while already and thought a blogpost would be a great way to demonstrate that.</p>
<p>In order to write this blog I’ve been reading carefully all individual packaages websites and this excellent <a href="https://www.alexpghayes.com/blog/implementing-the-super-learner-with-tidymodels/">blogpost</a> from Alex Hayes helped me a lot to put things together.</p>
</div>
<div id="simple-exploration" class="section level1">
<h1>Simple exploration</h1>
<p>Let’s load all the required packages and <code>credit_data</code> dataset available from <code>recipes</code> that we will use for modelling.</p>
<pre class="r"><code>set.seed(42)

library(tidymodels)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;xts&#39;:
##   method     from
##   as.zoo.xts zoo</code></pre>
<pre><code>## ── Attaching packages ───────────────────────────────────── tidymodels 0.0.2 ──</code></pre>
<pre><code>## ✔ broom     0.5.2          ✔ purrr     0.3.2     
## ✔ dials     0.0.2          ✔ recipes   0.1.6.9000
## ✔ dplyr     0.8.1          ✔ rsample   0.0.5     
## ✔ ggplot2   3.2.0          ✔ tibble    2.1.3     
## ✔ infer     0.4.0.1        ✔ yardstick 0.0.3     
## ✔ parsnip   0.0.3</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────── tidymodels_conflicts() ──
## ✖ purrr::discard() masks scales::discard()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()
## ✖ recipes::step()  masks stats::step()</code></pre>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;rvest&#39;:
##   method            from
##   read_xml.response xml2</code></pre>
<pre><code>## ── Attaching packages ────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ readr   1.3.1     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────── tidyverse_conflicts() ──
## ✖ readr::col_factor() masks scales::col_factor()
## ✖ purrr::discard()    masks scales::discard()
## ✖ dplyr::filter()     masks stats::filter()
## ✖ stringr::fixed()    masks recipes::fixed()
## ✖ dplyr::lag()        masks stats::lag()
## ✖ readr::spec()       masks yardstick::spec()</code></pre>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:yardstick&#39;:
## 
##     precision, recall</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre class="r"><code>library(magrittr)</code></pre>
<pre><code>## 
## Attaching package: &#39;magrittr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     extract</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     set_names</code></pre>
<pre class="r"><code>library(naniar)
library(furrr)</code></pre>
<pre><code>## Loading required package: future</code></pre>
<pre><code>## 
## Attaching package: &#39;future&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:caret&#39;:
## 
##     cluster</code></pre>
<pre class="r"><code>plan(multicore)  
data(&quot;credit_data&quot;)</code></pre>
<p>In this example I’m building a classification model to distinguish between good and bad loans indicated with column ‘Status’. We have relatively many observations compared to the number of variables available for modelling. I’m converting all columns to lowercase.</p>
<pre class="r"><code>glimpse(credit_data)</code></pre>
<pre><code>## Observations: 4,454
## Variables: 14
## $ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, b…
## $ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15…
## $ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, …
## $ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36…
## $ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37…
## $ Marital   &lt;fct&gt; married, widow, married, single, single, married, marr…
## $ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, n…
## $ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixe…
## $ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75…
## $ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 1…
## $ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3…
## $ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0…
## $ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200,…
## $ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 14…</code></pre>
<pre class="r"><code>credit_data %&lt;&gt;%
  set_names(., tolower(names(.)))</code></pre>
<p>With the help of <code>naniar</code> I’m checking the percentage of missing data per each variable. For this particular dataset there are very few missing values so they won’t pose a problem for us in modelling.</p>
<pre class="r"><code>credit_data %&gt;% miss_var_summary()</code></pre>
<pre><code>## # A tibble: 14 x 3
##    variable  n_miss pct_miss
##    &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;
##  1 income       381   8.55  
##  2 assets        47   1.06  
##  3 debt          18   0.404 
##  4 home           6   0.135 
##  5 job            2   0.0449
##  6 marital        1   0.0225
##  7 status         0   0     
##  8 seniority      0   0     
##  9 time           0   0     
## 10 age            0   0     
## 11 records        0   0     
## 12 expenses       0   0     
## 13 amount         0   0     
## 14 price          0   0</code></pre>
<p>In credit scoring problems we typically deal with something called a target <a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">class imbalace</a>, but in this particular case it’s not that severe. For the sake of comparing programming frameworks and not implementing the best ML model I will ignore it.</p>
<pre class="r"><code>table(credit_data$status)</code></pre>
<pre><code>## 
##  bad good 
## 1254 3200</code></pre>
<pre class="r"><code>round(prop.table(table(credit_data$status)), 2)</code></pre>
<pre><code>## 
##  bad good 
## 0.28 0.72</code></pre>
</div>
<div id="data-preparation" class="section level1">
<h1>Data preparation</h1>
<p>Let’s finally move on and start modelling! In the beginning we’ll start with dividing our dataset into training and testing with the help of the <code>rsample</code> package. It set’s an initial, stratified split where 80% of the data is dedicated to training and the rest to evaluating both models.</p>
<p>Furthermore, I’m creating cross-validation splits from the testing data of 5 folds. For compatibility with <code>caret</code> I’m using the <code>rsample2caret</code> function to make use of the same splits - otherwise both solutions wouldn’t be 100% comparable</p>
<pre class="r"><code>split &lt;- initial_split(credit_data, prop = 0.80, strata = &quot;status&quot;)

df_train &lt;- training(split)
df_test  &lt;- testing(split)

train_cv &lt;- vfold_cv(df_train, v = 5, strata = &quot;status&quot;)
train_cv_caret &lt;- rsample2caret(train_cv)</code></pre>
<p>I would like to fit a Random Forest model for which I specify a simple recipe. Tree-based models required very little preprocessing and in this particular example I mainly focus on imputting missing data or assigning them a new categorical level, infrequent/ unobserved values and dummyfying them. The same recipe will be used for both: <code>caret</code> and <code>tidymodels</code> model.</p>
<p>Normally I would also do some feature engineering, try to assess potential interactions and so on but I will write a separate post dedicated to that.</p>
<pre class="r"><code>recipe &lt;- df_train %&gt;%
  recipe(status ~ .) %&gt;%

  # Imputation
  step_unknown(all_nominal(), -status) %&gt;% 
  step_medianimpute(all_numeric()) %&gt;%

  # Nominal variables sanity check
  step_other(all_nominal(), -status, other = &quot;infrequent_combined&quot;) %&gt;%
  step_novel(all_nominal(), -status, new_level = &quot;unrecorded_observation&quot;) %&gt;%

  # Hot-encoding
  step_dummy(all_nominal(), -status, one_hot = TRUE) %&gt;%

  # Taking care of output consistency
  step_nzv(all_predictors()) %&gt;%
  check_missing(all_predictors())</code></pre>
<p>Let’s take a quick look at the output of the recipe:</p>
<pre class="r"><code>(recipe_preped &lt;- prep(recipe, retain = TRUE))</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         13
## 
## Training data contained 3565 data points and 333 incomplete rows. 
## 
## Operations:
## 
## Unknown factor level assignment for home, marital, records, job [trained]
## Median Imputation for seniority, time, age, expenses, ... [trained]
## Collapsing factor levels for home, marital, records, job [trained]
## Novel factor level assignment for home, marital, records, job [trained]
## Dummy variables from home, marital, records, job [trained]
## Sparse, unbalanced variable filter removed debt, ... [trained]
## Check missing values for seniority, time, age, expenses, ... [trained]</code></pre>
<pre class="r"><code>juice(recipe_preped) %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 3,565
## Variables: 22
## $ seniority                   &lt;int&gt; 9, 17, 10, 0, 1, 29, 9, 6, 8, 19, 0,…
## $ time                        &lt;int&gt; 60, 60, 36, 36, 60, 60, 12, 48, 60, …
## $ age                         &lt;int&gt; 30, 58, 46, 26, 36, 44, 27, 34, 30, …
## $ expenses                    &lt;int&gt; 73, 48, 90, 46, 75, 75, 35, 60, 75, …
## $ income                      &lt;dbl&gt; 129, 131, 200, 107, 214, 125, 80, 12…
## $ assets                      &lt;dbl&gt; 0, 0, 3000, 0, 3500, 10000, 0, 4000,…
## $ amount                      &lt;int&gt; 800, 1000, 2000, 310, 650, 1600, 200…
## $ price                       &lt;int&gt; 846, 1658, 2985, 910, 1645, 1800, 10…
## $ status                      &lt;fct&gt; good, good, bad, good, good, good, g…
## $ home_other                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …
## $ home_owner                  &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, …
## $ home_parents                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …
## $ home_priv                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …
## $ home_rent                   &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ marital_married             &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, …
## $ marital_single              &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, …
## $ marital_infrequent_combined &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ records_no                  &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, …
## $ records_yes                 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, …
## $ job_fixed                   &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, …
## $ job_freelance               &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, …
## $ job_partime                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …</code></pre>
<pre class="r"><code>tidy(recipe_preped)</code></pre>
<pre><code>## # A tibble: 7 x 6
##   number operation type         trained skip  id                
##    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;             
## 1      1 step      unknown      TRUE    FALSE unknown_5CFHt     
## 2      2 step      medianimpute TRUE    FALSE medianimpute_Hkncr
## 3      3 step      other        TRUE    FALSE other_tZVMD       
## 4      4 step      novel        TRUE    FALSE novel_KC08V       
## 5      5 step      dummy        TRUE    FALSE dummy_05QZl       
## 6      6 step      nzv          TRUE    FALSE nzv_pfTs9         
## 7      7 check     missing      TRUE    FALSE missing_I9k4v</code></pre>
</div>
<div id="fitting-our-models" class="section level1">
<h1>Fitting our models</h1>
<div id="caret" class="section level2">
<h2>Caret</h2>
<p>In the code below I’m setting control parameters for the <code>caret</code> model fit as well as the grid of hyperparameters that will be assessed in order to pick the best combination. Note that I’m using the very original observation indexes for cross-validation to ensure reproducability. The control function will also ensure that final hold-out predictions from cross-validation will be persisted for further assessment thanks to <code>savePredictions = &quot;final&quot;</code>.</p>
<p>We have 5 different CV folds and 30 grid combinations to assess which results in 150 models that will be fit and each comprising of 500 individual trees! All models will be assessed based on the <code>prSummary</code> function which is know as the AUC.</p>
<pre><code>##    mtry  splitrule min.node.size
## 1     1 extratrees             1
## 2     4 extratrees             1
## 3     7 extratrees             1
## 4    10 extratrees             1
## 5    13 extratrees             1
## 6     1       gini             1
## 7     4       gini             1
## 8     7       gini             1
## 9    10       gini             1
## 10   13       gini             1
## 11    1 extratrees             3
## 12    4 extratrees             3
## 13    7 extratrees             3
## 14   10 extratrees             3
## 15   13 extratrees             3
## 16    1       gini             3
## 17    4       gini             3
## 18    7       gini             3
## 19   10       gini             3
## 20   13       gini             3
## 21    1 extratrees             5
## 22    4 extratrees             5
## 23    7 extratrees             5
## 24   10 extratrees             5
## 25   13 extratrees             5
## 26    1       gini             5
## 27    4       gini             5
## 28    7       gini             5
## 29   10       gini             5
## 30   13       gini             5</code></pre>
<p>The great advantage of <code>caret</code> is that it wraps a lot of small code pieces in just one, high-level API call that does all the job for you - fit’s all individual models across CV folds and resamples, selects the best one and fits it already on the entire training dataset. It also makes sure it’s done as fast as possible thanks to parallel processing whenever it’s an option.</p>
<p>The drawback on the other hand is that it’s quite monolythic, untidy and at the end doesn’t offer a great deal of granularity to the end user.</p>
<pre class="r"><code>(model_caret &lt;- train(
  status ~ .,
  data = juice(recipe_preped),
  method = &quot;ranger&quot;,
  metric = &quot;ROC&quot;,
  trControl = control_caret,
  tuneGrid = grid_caret,
  importance = &quot;impurity&quot;,
  num.trees = 500
  ))</code></pre>
<pre><code>## Warning in train.default(x, y, weights = w, ...): The metric &quot;ROC&quot; was not
## in the result set. AUC will be used instead.</code></pre>
<pre><code>## Random Forest 
## 
## 3565 samples
##   21 predictor
##    2 classes: &#39;bad&#39;, &#39;good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 2851, 2852, 2852, 2852, 2853 
## Resampling results across tuning parameters:
## 
##   mtry  splitrule   min.node.size  AUC        Precision  Recall    
##    1    extratrees  1              0.6061115  0.7838095  0.06474129
##    1    extratrees  3              0.6051616  0.7972727  0.07071144
##    1    extratrees  5              0.6027262  0.8007190  0.06873134
##    1    gini        1              0.6482397  0.8240668  0.12051741
##    1    gini        3              0.6485132  0.8283575  0.11654229
##    1    gini        5              0.6463277  0.8156980  0.11953234
##    4    extratrees  1              0.6152929  0.6577883  0.44226368
##    4    extratrees  3              0.6173112  0.6668512  0.44624876
##    4    extratrees  5              0.6194269  0.6615736  0.44326368
##    4    gini        1              0.6458236  0.6893425  0.47812935
##    4    gini        3              0.6490075  0.6904558  0.47611940
##    4    gini        5              0.6486195  0.6832797  0.47411940
##    7    extratrees  1              0.6208018  0.6530371  0.46516915
##    7    extratrees  3              0.6213711  0.6585596  0.46815920
##   F        
##   0.1192973
##   0.1295330
##   0.1265205
##   0.2093612
##   0.2039280
##   0.2073836
##   0.5284667
##   0.5343328
##   0.5303940
##   0.5644752
##   0.5633072
##   0.5595421
##   0.5431642
##   0.5469773
##  [ reached getOption(&quot;max.print&quot;) -- omitted 16 rows ]
## 
## AUC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 4, splitrule = gini
##  and min.node.size = 3.</code></pre>
<p><code>Caret</code> also comes with built-in handy functions for assessing model’s individual predictors strength. By setting the <code>importance = &quot;impurity&quot;</code> in the <code>ranger</code> engine we ensure that variable importance will be returned by the final train object.</p>
<pre class="r"><code># Accessing most predictive attributes from caret 
varImp(model_caret, scale = TRUE)$importance %&gt;% 
  rownames_to_column() %&gt;% 
  arrange(-Overall)</code></pre>
<pre><code>##                        rowname    Overall
## 1                       income 100.000000
## 2                    seniority  92.313616
## 3                       amount  86.333956
## 4                        price  85.240770
## 5                          age  69.936889
## 6                       assets  59.459264
## 7                     expenses  47.983179
## 8                  records_yes  36.831735
## 9                         time  31.706665
## 10                  records_no  29.285685
## 11                 job_partime  19.955812
## 12                   job_fixed  18.532941
## 13                  home_owner  10.860895
## 14                   home_rent   6.732390
## 15               job_freelance   5.271521
## 16                home_parents   4.339822
## 17             marital_married   4.110193
## 18                  home_other   3.758374
## 19              marital_single   2.801483
## 20 marital_infrequent_combined   1.103223
## 21                   home_priv   0.000000</code></pre>
<p>Final cross-validated and test results are easily available with just a couple lines of code. Note that cross-validation performance is aggregated per each index (observation) and averaged out before the final performance metric is calculated.</p>
<p>Getting the test performance is a matter of baking the already prepped recipe on the training set and then making the prediction using the train object.</p>
<pre class="r"><code>df_train_pred_caret &lt;- model_caret$pred %&gt;% 
  group_by(rowIndex) %&gt;% 
  summarise(bad = mean(bad)) %&gt;% 
  transmute(estimate = bad) %&gt;% 
  add_column(truth = df_train$status)

df_test_pred_caret &lt;- predict(
    model_caret,
    newdata = bake(recipe_preped, df_test),
    type = &quot;prob&quot;) %&gt;%
  as_tibble() %&gt;%
  transmute(estimate = bad) %&gt;%
  add_column(truth = df_test$status)

print(&quot;Cross-validated training performance&quot;)</code></pre>
<pre><code>## [1] &quot;Cross-validated training performance&quot;</code></pre>
<pre class="r"><code>percent(roc_auc(df_train_pred_caret, truth, estimate)$.estimate)</code></pre>
<pre><code>## [1] &quot;82.8%&quot;</code></pre>
<pre class="r"><code>print(&quot;Test performance&quot;)</code></pre>
<pre><code>## [1] &quot;Test performance&quot;</code></pre>
<pre class="r"><code>percent(roc_auc(df_test_pred_caret, truth, estimate)$.estimate)</code></pre>
<pre><code>## [1] &quot;82.3%&quot;</code></pre>
</div>
<div id="tidymodels" class="section level2">
<h2>Tidymodels</h2>
<p>When I first saw some of the very first articles about doing ML the tidy way by combining <code>recipes</code> and <code>rsample</code> my thoughts were that it was all way too complicated compared to what <code>caret</code> offers. I was very surprised now when I got to know how clean and simple it became, and apparently things will be further simplified over the next months (<a href="https://github.com/tidymodels/parsnip/issues/200">link</a>)!</p>
<p>First let’s define two helper functions that will be used later during the modelling process:</p>
<pre class="r"><code># Defining helper functions that will be used later on
fit_on_fold &lt;- function(spec, prepped) {
  
  x &lt;- juice(prepped, all_predictors())
  y &lt;- juice(prepped, all_outcomes())
  
  fit_xy(spec, x, y)
}

predict_helper &lt;- function(split, recipe, fit) {
  
  new_x &lt;- bake(recipe, new_data = assessment(split), all_predictors())
  
  predict(fit, new_x, type = &quot;prob&quot;) %&gt;% 
    bind_cols(assessment(split) %&gt;% select(status)) 
}</code></pre>
<p>First, let’s use <code>parsnip</code> to define our ‘modelling engine’ - just like before we’re setting it as a classification problem, using Random Forest running on the <code>ranger</code> engine. On top of that I’m using <code>dials</code> to define a grid of parameters to optimize. <code>Dials</code> provide a set of handy functions, such as: <code>grid_random</code> or <code>grid_regular</code>, that let you choose the range of parameters in a very flexible way.</p>
<p>From what I can see the parameters that could be optimized differ slighlty between both frameworks: <code>caret</code> allows for tunning the ‘min.node.size’ while keeping the ‘trees’ constant, while <code>parsnip</code> allows for tuning ‘trees’ while keeping ‘min.node.size’ constant (I assume it’s choosing the default <code>ranger</code> values). Nevertheless, the total amount of combinations is same in both cases and equal to 30.</p>
<pre class="r"><code># Specifying the modelling engine
(engine_tidym &lt;- rand_forest(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;ranger&quot;))</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Computational engine: ranger</code></pre>
<pre class="r"><code># Specifying the grid of hyperparameters that should be tested
(gridy_tidym &lt;- grid_random(
  mtry %&gt;% range_set(c( 1,  20)),
  trees %&gt;% range_set(c( 500, 1000)), 
  min_n %&gt;% range_set(c(2,  10)),
  size = 30
  ))</code></pre>
<pre><code>## # A tibble: 30 x 3
##     mtry trees min_n
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1    15   869     5
##  2    17   774     3
##  3    18   785     3
##  4    20   789    10
##  5     6   970     9
##  6     2   854     8
##  7    11   571     4
##  8    19   564     3
##  9     9   779     3
## 10     9   735     5
## # … with 20 more rows</code></pre>
<p>Now comes the really interesting part of <code>tidymodels</code>: we’re using a <code>merge</code> helper function from <code>dials</code> to bind our predefined ‘modelling engine’ with all grid combinations of the hyperparameters to tune.</p>
<pre class="r"><code>merge(engine_tidym, gridy_tidym)[1:3]</code></pre>
<pre><code>## [[1]]
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 15
##   trees = 869
##   min_n = 5
## 
## Computational engine: ranger 
## 
## 
## [[2]]
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 17
##   trees = 774
##   min_n = 3
## 
## Computational engine: ranger 
## 
## 
## [[3]]
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 18
##   trees = 785
##   min_n = 3
## 
## Computational engine: ranger</code></pre>
<p>Subsequently, I’m putting it into a tidy, data frame structure where each model-parameters combination is bound together and assigned a model id that will be used later to make a distinction between consequtive fits.</p>
<pre class="r"><code># Merging all possibilities with our cross-validated data frame
(spec_tidym &lt;- tibble(spec = merge(engine_tidym, gridy_tidym)) %&gt;% 
  mutate(model_id = row_number()))</code></pre>
<pre><code>## # A tibble: 30 x 2
##    spec      model_id
##    &lt;list&gt;       &lt;int&gt;
##  1 &lt;spec[+]&gt;        1
##  2 &lt;spec[+]&gt;        2
##  3 &lt;spec[+]&gt;        3
##  4 &lt;spec[+]&gt;        4
##  5 &lt;spec[+]&gt;        5
##  6 &lt;spec[+]&gt;        6
##  7 &lt;spec[+]&gt;        7
##  8 &lt;spec[+]&gt;        8
##  9 &lt;spec[+]&gt;        9
## 10 &lt;spec[+]&gt;       10
## # … with 20 more rows</code></pre>
<p>Lastly, I’m adding the last component into this tidy structure: all cross-validation splits that were specified before with the use of the <code>crossing</code> function. This part is very likely to evolve and be simplified in the upcoming months. Now we’re all set to start the actual modelling!</p>
<pre class="r"><code>(spec_tidym &lt;- crossing(train_cv, spec_tidym))</code></pre>
<pre><code>## # A tibble: 150 x 4
##    splits             id    spec      model_id
##    &lt;named list&gt;       &lt;chr&gt; &lt;list&gt;       &lt;int&gt;
##  1 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        1
##  2 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        2
##  3 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        3
##  4 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        4
##  5 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        5
##  6 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        6
##  7 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        7
##  8 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        8
##  9 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;        9
## 10 &lt;split [2.9K/714]&gt; Fold1 &lt;spec[+]&gt;       10
## # … with 140 more rows</code></pre>
<p>To speed thigs up let’s use the <code>multicore</code> setting in the <code>furrr</code> package and fit many models simultaneously. In the following code our original recipe is first prepped on each split’s training set and than it’s used by the <code>fit_on_fold</code> helper function to fit a given model-parameter combination.</p>
<pre class="r"><code># Fitting each model-fold pair
fits_tidym &lt;- spec_tidym %&gt;%
  mutate(
    prepped = future_map(splits, prepper, recipe),
    fit = future_map2(spec, prepped, fit_on_fold)
  )</code></pre>
<p>The last step of modelling involves usage of the other <code>predict_helper</code> function that bakes the already prepped split recipe and applies it on the testing set of the split in order to make a prediction of the given model-parameters combination.</p>
<pre class="r"><code># Making predictions of each fitted model on the testing set
fits_pred_tidym &lt;- fits_tidym %&gt;%
  mutate(
    preds = future_pmap(list(splits, prepped, fit), predict_helper)
  )</code></pre>
<p>After training is done I would like to assess which model performs the best based on cross-validated hold-out sets performance. In order to do that let’s calculate the AUC of all test sets across all model-parameters combinations. By averaging the results up, I can see the entire performance profile of all possibilities.</p>
<pre class="r"><code># Assessing individual model-fold performance and averaging performance across all folds for each model
(perf_summary_tidym &lt;- fits_pred_tidym %&gt;% 
  unnest(preds) %&gt;% 
  group_by(id, model_id) %&gt;% 
  roc_auc(truth = status, .pred_bad) %&gt;% 
  group_by(model_id, .metric, .estimator) %&gt;% 
  summarize(mean = mean(.estimate, na.rm = TRUE))) %&gt;% 
  arrange(-mean)</code></pre>
<pre><code>## # A tibble: 30 x 4
## # Groups:   model_id, .metric [30]
##    model_id .metric .estimator  mean
##       &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;
##  1        6 roc_auc binary     0.831
##  2       19 roc_auc binary     0.831
##  3       25 roc_auc binary     0.831
##  4       22 roc_auc binary     0.830
##  5       28 roc_auc binary     0.830
##  6        5 roc_auc binary     0.829
##  7       21 roc_auc binary     0.829
##  8       23 roc_auc binary     0.828
##  9       13 roc_auc binary     0.827
## 10       15 roc_auc binary     0.827
## # … with 20 more rows</code></pre>
<p>It’s easy to see that the best performing model is the one of id 3. Let’s now take a step back and filter only that model specification, and fit it on the entire training set. As of now I’m not 100% sure what the recommended and most efficient way of doing that would be, but I decided to go for something like that:</p>
<pre class="r"><code># Selecting the best model with:
# perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]

# Fitting the best model on the full training set
(model_tidym &lt;- tibble(spec = merge(engine_tidym, gridy_tidym)) %&gt;% 
  mutate(model_id = row_number()) %&gt;% 
  filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %&gt;% 
  pull(spec) %&gt;% 
  .[[1]] %&gt;% 
  fit(status ~ ., juice(recipe_preped)))</code></pre>
<pre><code>## parsnip model object
## 
## Ranger result
## 
## Call:
##  ranger::ranger(formula = formula, data = data, mtry = ~2L, num.trees = ~854L,      min.node.size = ~8L, num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  854 
## Sample size:                      3565 
## Number of independent variables:  21 
## Mtry:                             2 
## Target node size:                 8 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.145983</code></pre>
<p>Similarly like before with <code>caret</code> I can now summarize our cross-validated and test performances:</p>
<pre class="r"><code>print(&quot;Cross-validated training performance&quot;)</code></pre>
<pre><code>## [1] &quot;Cross-validated training performance&quot;</code></pre>
<pre class="r"><code>percent(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])</code></pre>
<pre><code>## [1] &quot;83.1%&quot;</code></pre>
<pre class="r"><code>print(&quot;Test performance&quot;)</code></pre>
<pre><code>## [1] &quot;Test performance&quot;</code></pre>
<pre class="r"><code>df_train_pred_tidym &lt;- predict(
  model_tidym, 
  new_data = bake(recipe_preped, df_test), 
  type = &quot;prob&quot;
  ) %&gt;% 
  transmute(estimate = .pred_bad) %&gt;%
  add_column(truth = df_test$status)

percent(roc_auc(df_train_pred_tidym, truth, estimate)$.estimate)</code></pre>
<pre><code>## [1] &quot;82.1%&quot;</code></pre>
<p>This entire <code>tidymodels</code> code that was scattered across all above sections could be easily squeezed in one longer pipeline. Note that I limited the grid to just one row <code>gridy_tidym[1, ]</code> in order to demonstrate the solution and save on processing time.</p>
<pre class="r"><code>df_tidym &lt;- tibble(spec = merge(engine_tidym, gridy_tidym[1:2, ])) %&gt;% 
  mutate(model_id = row_number()) %&gt;% 
  crossing(train_cv, .) %&gt;%
  mutate(
    prepped = future_map(splits, prepper, recipe),
    fit = future_map2(spec, prepped, fit_on_fold),
    preds = future_pmap(list(splits, prepped, fit), predict_helper)
  )</code></pre>
</div>
</div>
<div id="conslusions" class="section level1">
<h1>Conslusions</h1>
</div>
<div id="future-considerations" class="section level1">
<h1>Future considerations</h1>
</div>
