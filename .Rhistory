juice(recipe_preped) %>% glimpse()
bake(recipe_preped, df_test)
model_caret
predict(
model_caret,
newdata = bake(recipe_preped, df_test),
type = "prob")
test <- bake(recipe_preped, df_test)
predict(
model_caret,
newdata = test,
type = "prob")
model_caret
predict(
model_caret,
newdata = bake(recipe_preped, df_test),
type = "prob")
set.seed(42)
library(tidymodels)
library(tidyverse)
library(caret)
library(magrittr)
library(naniar)
data("credit_data")
glimpse(credit_data)
credit_data %<>%
set_names(., tolower(names(.)))
glimpse(credit_data)
credit_data %<>%
set_names(., tolower(names(.)))
credit_data %>% miss_var_summary()
table(credit_data$status)
split <- initial_split(credit_data, prop = 0.80, strata = "status")
df_train <- training(split)
df_test  <- testing(split)
train_cv <- vfold_cv(df_train, v = 5, strata = "status")
train_cv_caret <- rsample2caret(train_cv)
vec_nominal <- c("home", "marital", "records", "job")
recipe <- df_train %>%
recipe(status ~ .) %>%
# Imputation
step_modeimpute(one_of(vec_nominal)) %>%
step_medianimpute(all_numeric(), -one_of(vec_nominal)) %>%
# Nominal variables sanity check
step_other(one_of(vec_nominal), # this function doesn't respect types?
other = "infrequent_combined") %>%
step_novel(one_of(vec_nominal),
new_level = "unrecorded_observation") %>%
# One-hot encoding
step_dummy(one_of(vec_nominal), one_hot = TRUE) %>%
# Taking care of output consistency
step_nzv(all_predictors()) %>%
check_missing(all_predictors())
(recipe_preped <- prep(recipe, retain = TRUE))
juice(recipe_preped) %>% glimpse()
tidy(recipe_preped)
control_caret <- trainControl(
method = "cv",
verboseIter = FALSE,
classProbs = TRUE,
summaryFunction = twoClassSummary,
returnResamp = "final",
savePredictions = "final",
index = train_cv_caret$index,
indexOut = train_cv_caret$indexOut,
)
grid_caret <- expand.grid(
mtry = seq(1, ncol(df_train) - 1, 3),
splitrule = c("extratrees", "gini"),
min.node.size = c(1, 3, 5)
)
model_caret <- train(
recipe,
data = df_train,
method = "ranger",
metric = "ROC",
trControl = control_caret,
tuneGrid = grid_caret,
importance = "impurity",
num.trees = 500
)
control_caret <- trainControl(
method = "cv",
verboseIter = FALSE,
classProbs = TRUE,
summaryFunction = twoClassSummary,
returnResamp = "final",
savePredictions = "final",
index = train_cv_caret$index,
indexOut = train_cv_caret$indexOut,
)
grid_caret <- expand.grid(
mtry = seq(1, ncol(df_train) - 1, 3),
splitrule = c("extratrees", "gini"),
min.node.size = c(1, 3, 5)
)
model_caret <- train(
recipe,
data = df_train,
method = "ranger",
metric = "ROC",
trControl = control_caret,
tuneGrid = grid_caret,
importance = "impurity",
num.trees = 500
)
set.seed(42)
library(tidymodels)
library(tidyverse)
library(caret)
library(magrittr)
library(naniar)
library(furrr)
data("credit_data")
glimpse(credit_data)
credit_data %<>%
set_names(., tolower(names(.)))
credit_data %>% miss_var_summary()
table(credit_data$status)
split <- initial_split(credit_data, prop = 0.80, strata = "status")
df_train <- training(split)
df_test  <- testing(split)
train_cv <- vfold_cv(df_train, v = 5, strata = "status")
train_cv_caret <- rsample2caret(train_cv)
recipe <- df_train %>%
recipe(status ~ .) %>%
# Imputation
step_modeimpute(all_nominal(), -status) %>%
step_medianimpute(all_numeric()) %>%
# Nominal variables sanity check
step_other(all_nominal(), -status,
other = "infrequent_combined") %>%
step_novel(all_nominal(), -status,
new_level = "unrecorded_observation") %>%
# One-hot encoding
step_dummy(all_nominal(), -status, one_hot = TRUE) %>%
# Taking care of output consistency
step_nzv(all_predictors()) %>%
check_missing(all_predictors())
(recipe_preped <- prep(recipe, retain = TRUE))
juice(recipe_preped) %>% glimpse()
tidy(recipe_preped)
control_caret <- trainControl(
method = "cv",
verboseIter = FALSE,
classProbs = TRUE,
summaryFunction = twoClassSummary,
returnResamp = "final",
savePredictions = "final",
index = train_cv_caret$index,
indexOut = train_cv_caret$indexOut,
)
grid_caret <- expand.grid(
mtry = seq(1, ncol(df_train) - 1, 3),
splitrule = c("extratrees", "gini"),
min.node.size = c(1, 3, 5)
)
(model_caret <- train(
status ~ .,
data = juice(recipe_preped),
method = "ranger",
metric = "ROC",
trControl = control_caret,
tuneGrid = grid_caret,
importance = "impurity",
num.trees = 500
))
# model_caret <- train( # problem during prediction
#   recipe,
#   data = df_train,
#   method = "ranger",
#   metric = "ROC",
#   trControl = control_caret,
#   tuneGrid = grid_caret,
#   importance = "impurity",
#   num.trees = 500
#   )
varImp(model_caret, scale = TRUE)$importance %>%
rownames_to_column() %>%
arrange(-Overall)
df_train_pred <- model_caret$pred %>%
group_by(rowIndex) %>%
summarise(bad = mean(bad)) %>%
transmute(estimate = bad) %>%
add_column(truth = df_train$status)
df_test_pred <- predict(
model_caret,
newdata = bake(recipe_preped, df_test),
type = "prob") %>%
as_tibble() %>%
transmute(estimate = bad) %>%
add_column(truth = df_test$status)
roc_auc(df_train_pred, truth, estimate)
roc_auc(df_test_pred, truth, estimate)
fit_on_fold <- function(spec, prepped) {
x <- juice(prepped, all_predictors())
y <- juice(prepped, all_outcomes())
fit_xy(spec, x, y)
}
predict_helper <- function(split, recipe, fit) {
new_x <- bake(recipe, new_data = assessment(split), all_predictors())
predict(fit, new_x, type = "prob") %>%
bind_cols(assessment(split) %>% select(Species))
}
engine_tidym <- rand_forest(mode = "classification") %>%
set_engine("ranger")
gridy_tidym <- grid_random(
mtry %>% range_set(c( 1,  20)),
trees %>% range_set(c( 500, 1000)),
min_n %>% range_set(c(2,  10)),
size = 20
)
spec_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number())
spec_tidym <- crossing(train_cv, spec_tidym)
fits_tidym <- spec_tidym %>%
mutate(
prepped = future_map(splits, prepper, recipe),
fit = future_map2(spec, prepped, fit_on_fold)
)
fits_pred_tidym <- fits_tidym %>%
mutate(
preds = future_pmap(list(splits, prepped, fit), predict_helper)
)
predict_helper <- function(split, recipe, fit) {
new_x <- bake(recipe, new_data = assessment(split), all_predictors())
predict(fit, new_x, type = "prob") %>%
bind_cols(assessment(split) %>% select(status))
}
fits_pred_tidym <- fits_tidym %>%
mutate(
preds = future_pmap(list(splits, prepped, fit), predict_helper)
)
fits_pred_tidym
indiv_estimates <- fits_pred_tidym %>%
unnest(preds) %>%
group_by(id, model_id)
indiv_estimates
?mn_log_loss
?roc_auc()
indiv_estimates <- fits_pred_tidym %>%
unnest(preds) %>%
group_by(id, model_id) %>%
roc_auc(truth = status, .pred_bad)
indiv_estimates
rs_estimates <- indiv_estimates %>%
group_by(model_id, .metric, .estimator) %>%
summarize(mean = mean(.estimate, na.rm = TRUE))
rs_estimates
df_train_pred
perf_summary_tidym <- fits_pred_tidym %>%
unnest(preds) %>%
group_by(id, model_id) %>%
roc_auc(truth = status, .pred_bad) %>%
group_by(model_id, .metric, .estimator) %>%
summarize(mean = mean(.estimate, na.rm = TRUE))
perf_summary_tidym
which.max(perf_summary_tidym$mean)
perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]
spec_tidym
merge(engine_tidym, gridy_tidym)
tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number())
fits_pred_tidym
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)])
model_tidym
model_tidym$spec
perf_summary_tidym
perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)]
model_tidym
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec)
model_tidym
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec) %>%
fit(status ~ ., juice(recipe_preped))
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec)
model_tidym
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec) %>%
.[[1]]
model_tidym
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec) %>%
.[[1]] %>%
fit(status ~ ., juice(recipe_preped))
model_tidym
df_train_pred_caret <- model_caret$pred %>%
group_by(rowIndex) %>%
summarise(bad = mean(bad)) %>%
transmute(estimate = bad) %>%
add_column(truth = df_train$status)
df_test_pred_caret <- predict(
model_caret,
newdata = bake(recipe_preped, df_test),
type = "prob") %>%
as_tibble() %>%
transmute(estimate = bad) %>%
add_column(truth = df_test$status)
# Cross-validated training performance
roc_auc(df_train_pred_caret, truth, estimate)
# Test performance
roc_auc(df_test_pred_caret, truth, estimate)
perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)]
# Test performance
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob")
df_train_pred_tidym
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = bad) %>%
add_column(truth = df_test$status)
# Test performance
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
roc_auc(df_train_pred_tidym, truth, estimate)
perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)]
print("Cross-validated training performance")
(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])
print("Test performance")
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
roc_auc(df_train_pred_tidym, truth, estimate)
print("Cross-validated training performance")
(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])
print("Test performance")
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
(roc_auc(df_train_pred_tidym, truth, estimate))
print("Cross-validated training performance")
(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])
print("Test performance")
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
roc_auc_vec(df_train_pred_tidym, truth, estimate)
print("Cross-validated training performance")
(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])
print("Test performance")
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
print(roc_auc(df_train_pred_tidym, truth, estimate))
str(roc_auc(df_train_pred_tidym, truth, estimate))
?roc_auc
roc_auc(df_train_pred_tidym, truth, estimate)$.estimate
print("Cross-validated training performance")
(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])
print("Test performance")
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
roc_auc(df_train_pred_tidym, truth, estimate)$.estimate
fits_pred_tidym <- fits_tidym %>%
mutate(
preds = future_pmap(list(splits, prepped, fit), predict_helper)
)
perf_summary_tidym <- fits_pred_tidym %>%
unnest(preds) %>%
group_by(id, model_id) %>%
roc_auc(truth = status, .pred_bad) %>%
group_by(model_id, .metric, .estimator) %>%
summarize(mean = mean(.estimate, na.rm = TRUE))
# Selecting the best model
# perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]
model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec) %>%
.[[1]] %>%
fit(status ~ ., juice(recipe_preped))
fits_pred_tidym <- fits_tidym %>%
mutate(
preds = future_pmap(list(splits, prepped, fit), predict_helper)
)
perf_summary_tidym <- fits_pred_tidym %>%
unnest(preds) %>%
group_by(id, model_id) %>%
roc_auc(truth = status, .pred_bad) %>%
group_by(model_id, .metric, .estimator) %>%
summarize(mean = mean(.estimate, na.rm = TRUE))
# Selecting the best model with:
# perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]
(model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec) %>%
.[[1]] %>%
fit(status ~ ., juice(recipe_preped)))
# Selecting the best model with:
# perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]
(model_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()) %>%
filter(model_id == perf_summary_tidym$model_id[which.max(perf_summary_tidym$mean)]) %>%
pull(spec) %>%
.[[1]] %>%
fit(status ~ ., juice(recipe_preped)))
(perf_summary_tidym <- fits_pred_tidym %>%
unnest(preds) %>%
group_by(id, model_id) %>%
roc_auc(truth = status, .pred_bad) %>%
group_by(model_id, .metric, .estimator) %>%
summarize(mean = mean(.estimate, na.rm = TRUE)))
# Specifying the modelling engine
engine_tidym <- rand_forest(mode = "classification") %>%
set_engine("ranger")
# Specifying the grid of hyperparameters that should be tested
gridy_tidym <- grid_random(
mtry %>% range_set(c( 1,  20)),
trees %>% range_set(c( 500, 1000)),
min_n %>% range_set(c(2,  10)),
size = 20
)
# Merging all possibilities with our cross-validated data frame
spec_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number())
spec_tidym <- crossing(train_cv, spec_tidym)
(spec_tidym <- crossing(train_cv, spec_tidym))
# Merging all possibilities with our cross-validated data frame
(spec_tidym <- tibble(spec = merge(engine_tidym, gridy_tidym)) %>%
mutate(model_id = row_number()))
df_train_pred_caret <- model_caret$pred %>%
group_by(rowIndex) %>%
summarise(bad = mean(bad)) %>%
transmute(estimate = bad) %>%
add_column(truth = df_train$status)
df_test_pred_caret <- predict(
model_caret,
newdata = bake(recipe_preped, df_test),
type = "prob") %>%
as_tibble() %>%
transmute(estimate = bad) %>%
add_column(truth = df_test$status)
print("Cross-validated training performance")
roc_auc(df_train_pred_caret, truth, estimate)$.estimate
print("Test performance")
roc_auc(df_test_pred_caret, truth, estimate)$.estimate
percet(roc_auc(df_train_pred_caret, truth, estimate)$.estimate)
percent(roc_auc(df_train_pred_caret, truth, estimate)$.estimate)
percent(roc_auc(df_test_pred_caret, truth, estimate)$.estimate)
print("Cross-validated training performance")
percent(perf_summary_tidym$mean[which.max(perf_summary_tidym$mean)])
print("Test performance")
df_train_pred_tidym <- predict(
model_tidym,
new_data = bake(recipe_preped, df_test),
type = "prob"
) %>%
transmute(estimate = .pred_bad) %>%
add_column(truth = df_test$status)
percent(roc_auc(df_train_pred_tidym, truth, estimate)$.estimate)
df_train_pred_caret <- model_caret$pred %>%
group_by(rowIndex) %>%
summarise(bad = mean(bad)) %>%
transmute(estimate = bad) %>%
add_column(truth = df_train$status)
df_test_pred_caret <- predict(
model_caret,
newdata = bake(recipe_preped, df_test),
type = "prob") %>%
as_tibble() %>%
transmute(estimate = bad) %>%
add_column(truth = df_test$status)
print("Cross-validated training performance")
percent(roc_auc(df_train_pred_caret, truth, estimate)$.estimate)
print("Test performance")
percent(roc_auc(df_test_pred_caret, truth, estimate)$.estimate)
